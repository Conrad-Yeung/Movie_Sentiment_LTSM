{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY4SK0xKAJgm"
   },
   "source": [
    "# RNN with LSTM with Own Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moNmVfuvnImW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext  import datasets\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSRL42Qgy8I8"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvW1RgfepCBq"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 100 #At minimum 128 - needs to contain all the words\n",
    "HIDDEN_DIM = 275 #Original 256\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQMmKUEisW4W"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           review\n",
       "sentiment        \n",
       "0           25000\n",
       "1           25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OK... so... I really like Kris Kristofferson and his usual easy going delivery of lines in his movies. Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly. But, Disappearance is his misstep. Holy Moly, this was a bad movie! <br /><br />I must give kudos to the cinematography and and the actors, including Kris, for trying their darndest to make sense from this goofy, confusing story! None of it made sense and Kris probably didn't understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about! <br /><br />I don't care that everyone on this movie was doing out of love for the project, or some such nonsense... I've seen low budget movies that had a plot for goodness sake! This had none, zilcho, nada, zippo, empty of reason... a complete waste of good talent, scenery and celluloid! <br /><br />I rented this piece of garbage for a buck, and I want my money back! I want my 2 hours back I invested on this Grade F waste of my time! Don't watch this movie, or waste 1 minute of your valuable time while passing through a room where it's playing or even open up the case that is holding the DVD! Believe me, you'll thank me for the advice!\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GnH64XvsV8n"
   },
   "source": [
    "Define the Label and Text field formatters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.legacy.data.Field(sequential=True,\n",
    "                  tokenize='spacy',\n",
    "                  include_lengths=True) # necessary for packed_padded_sequence\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('review', TEXT), ('sentiment', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path=\"movie_data.csv\", format='csv',\n",
    "    skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, and test partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WZ_4jiHVnMxN",
    "outputId": "dfa51c04-4845-44c3-f50b-d36d41f132b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 37500\n",
      "Num Valid: 10000\n",
      "Num Test: 2500\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = dataset.split(\n",
    "    split_ratio=[0.75, 0.05, 0.2],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "#One may want to vary the test, train split percentages\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Valid: {len(valid_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-TBwKWPslPa"
   },
   "source": [
    "Build the vocabulary based on the top \"VOCABULARY_SIZE\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e8uNrjdtn4A8",
    "outputId": "6cf499d7-7722-4da0-8576-ee0f218cc6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20002\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpEMNInXtZsb"
   },
   "source": [
    "The TEXT.vocab dictionary will contain the word counts and indices. The reason why the number of words is VOCABULARY_SIZE + 2 is that it contains to special tokens for padding and unknown words: `<unk>` and `<pad>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIQ_zfKLwjKm"
   },
   "source": [
    "Make dataset iterators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7JiHR1stHNF"
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
    "    sort_key=lambda x: len(x.review),\n",
    "    device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0pT_dMRvicQ"
   },
   "source": [
    "Testing the iterators (note that the number of rows depends on the longest document in the respective batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y8SP_FccutT0",
    "outputId": "fe33763a-4560-4dee-adee-31cc6c48b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([287, 100])\n",
      "Target vector size: torch.Size([100])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([50, 100])\n",
      "Target vector size: torch.Size([100])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([68, 100])\n",
      "Target vector size: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#Batch review matrix dimensions = Tokens by Batchsize\n",
    "#Batch sentiment matrix dimensions = Batchsize x 1\n",
    "\n",
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.review[0].size()}')\n",
    "    print(f'Target vector size: {batch.sentiment.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.review[0].size()}')\n",
    "    print(f'Target vector size: {batch.sentiment.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.review[0].size()}')\n",
    "    print(f'Target vector size: {batch.sentiment.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_grdW3pxCzz"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQIUm5EjxFNa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        #Here is a preliminary model using LSTM cell\n",
    "        \n",
    "        #The primary goal of this lab is to vary the dimensions of the embeddings and see the results\n",
    "        #The second task is to use a another RNN cell such as GRU and perform parameter tuning and report the results.\n",
    "        \n",
    "        #Xt -> Ht\n",
    "        #Ht-> yt (output)\n",
    "        #Repeat again and again\n",
    "        #t time steps = a lot more parameters\n",
    "        \n",
    "        #Convert Data -> Matrix\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) #Convert Input Data (bag of words)-> Matrix Format (with dimensions = embedding_dim)\n",
    "        \n",
    "        #Use RNN on embedding dimension (128 dim) -> Hidden Layers (hidden_dim - 256 dim) \n",
    "        #Look at nn.LSTM hyperparameter documentation GOAL 1\n",
    "        #Change nn.LSTM to nn.GRU (difference: number of parameters/architecture of network vary abit) GOAL 2\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim) #Change LSTM to GRU\n",
    "        \n",
    "        #Get output from hidden layer (scalar output)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) #Desired output is binary format (integer): Dimesnion = 1\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
    "        \n",
    "        #[sentence len, batch size, embedding size] => \n",
    "        #  output: [sentence len, batch size, hidden size]\n",
    "        #  hidden: [1, batch size, hidden size]\n",
    "        \n",
    "        #packed_output, (hidden, cell) = self.rnn(packed) #LSTM output = output, (h0,c0)\n",
    "        packed_output, hidden = self.rnn(packed) #GRU output = output, h0\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0)).view(-1) #Returns Integer (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv9Ny9di6VcI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5t1Afn4xO11"
   },
   "outputs": [],
   "source": [
    "#Know accuracy of model\n",
    "#Loss fxc \n",
    "def compute_binary_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.review\n",
    "            logits = model(text, text_lengths.cpu())\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            num_examples += batch_data.sentiment.size(0)\n",
    "            correct_pred += (predicted_labels.long() == batch_data.sentiment.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1836
    },
    "colab_type": "code",
    "id": "EABZM8Vo0ilB",
    "outputId": "5d45e293-9909-4588-e793-8dfaf72e5c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/375 | Cost: 0.6866\n",
      "Epoch: 001/015 | Batch 050/375 | Cost: 0.6921\n",
      "Epoch: 001/015 | Batch 100/375 | Cost: 0.6750\n",
      "Epoch: 001/015 | Batch 150/375 | Cost: 0.6732\n",
      "Epoch: 001/015 | Batch 200/375 | Cost: 0.6814\n",
      "Epoch: 001/015 | Batch 250/375 | Cost: 0.6546\n",
      "Epoch: 001/015 | Batch 300/375 | Cost: 0.5979\n",
      "Epoch: 001/015 | Batch 350/375 | Cost: 0.6132\n",
      "training accuracy: 71.13%\n",
      "valid accuracy: 70.87%\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 002/015 | Batch 000/375 | Cost: 0.5637\n",
      "Epoch: 002/015 | Batch 050/375 | Cost: 0.5358\n",
      "Epoch: 002/015 | Batch 100/375 | Cost: 0.6029\n",
      "Epoch: 002/015 | Batch 150/375 | Cost: 0.4852\n",
      "Epoch: 002/015 | Batch 200/375 | Cost: 0.5506\n",
      "Epoch: 002/015 | Batch 250/375 | Cost: 0.4774\n",
      "Epoch: 002/015 | Batch 300/375 | Cost: 0.4795\n",
      "Epoch: 002/015 | Batch 350/375 | Cost: 0.4797\n",
      "training accuracy: 77.53%\n",
      "valid accuracy: 76.75%\n",
      "Time elapsed: 1.03 min\n",
      "Epoch: 003/015 | Batch 000/375 | Cost: 0.4449\n",
      "Epoch: 003/015 | Batch 050/375 | Cost: 0.4279\n",
      "Epoch: 003/015 | Batch 100/375 | Cost: 0.4265\n",
      "Epoch: 003/015 | Batch 150/375 | Cost: 0.5503\n",
      "Epoch: 003/015 | Batch 200/375 | Cost: 0.4356\n",
      "Epoch: 003/015 | Batch 250/375 | Cost: 0.4708\n",
      "Epoch: 003/015 | Batch 300/375 | Cost: 0.3552\n",
      "Epoch: 003/015 | Batch 350/375 | Cost: 0.3440\n",
      "training accuracy: 79.80%\n",
      "valid accuracy: 79.06%\n",
      "Time elapsed: 1.52 min\n",
      "Epoch: 004/015 | Batch 000/375 | Cost: 0.4192\n",
      "Epoch: 004/015 | Batch 050/375 | Cost: 0.4825\n",
      "Epoch: 004/015 | Batch 100/375 | Cost: 0.3923\n",
      "Epoch: 004/015 | Batch 150/375 | Cost: 0.3360\n",
      "Epoch: 004/015 | Batch 200/375 | Cost: 0.4326\n",
      "Epoch: 004/015 | Batch 250/375 | Cost: 0.4372\n",
      "Epoch: 004/015 | Batch 300/375 | Cost: 0.4243\n",
      "Epoch: 004/015 | Batch 350/375 | Cost: 0.3247\n",
      "training accuracy: 82.84%\n",
      "valid accuracy: 81.96%\n",
      "Time elapsed: 2.02 min\n",
      "Epoch: 005/015 | Batch 000/375 | Cost: 0.3170\n",
      "Epoch: 005/015 | Batch 050/375 | Cost: 0.4555\n",
      "Epoch: 005/015 | Batch 100/375 | Cost: 0.4477\n",
      "Epoch: 005/015 | Batch 150/375 | Cost: 0.4002\n",
      "Epoch: 005/015 | Batch 200/375 | Cost: 0.2910\n",
      "Epoch: 005/015 | Batch 250/375 | Cost: 0.3019\n",
      "Epoch: 005/015 | Batch 300/375 | Cost: 0.3528\n",
      "Epoch: 005/015 | Batch 350/375 | Cost: 0.2919\n",
      "training accuracy: 85.37%\n",
      "valid accuracy: 84.07%\n",
      "Time elapsed: 2.55 min\n",
      "Epoch: 006/015 | Batch 000/375 | Cost: 0.3668\n",
      "Epoch: 006/015 | Batch 050/375 | Cost: 0.3680\n",
      "Epoch: 006/015 | Batch 100/375 | Cost: 0.5318\n",
      "Epoch: 006/015 | Batch 150/375 | Cost: 0.3868\n",
      "Epoch: 006/015 | Batch 200/375 | Cost: 0.2729\n",
      "Epoch: 006/015 | Batch 250/375 | Cost: 0.2931\n",
      "Epoch: 006/015 | Batch 300/375 | Cost: 0.3131\n",
      "Epoch: 006/015 | Batch 350/375 | Cost: 0.3402\n",
      "training accuracy: 86.35%\n",
      "valid accuracy: 84.51%\n",
      "Time elapsed: 3.06 min\n",
      "Epoch: 007/015 | Batch 000/375 | Cost: 0.3617\n",
      "Epoch: 007/015 | Batch 050/375 | Cost: 0.5069\n",
      "Epoch: 007/015 | Batch 100/375 | Cost: 0.3894\n",
      "Epoch: 007/015 | Batch 150/375 | Cost: 0.4336\n",
      "Epoch: 007/015 | Batch 200/375 | Cost: 0.4022\n",
      "Epoch: 007/015 | Batch 250/375 | Cost: 0.3804\n",
      "Epoch: 007/015 | Batch 300/375 | Cost: 0.3587\n",
      "Epoch: 007/015 | Batch 350/375 | Cost: 0.3862\n",
      "training accuracy: 86.95%\n",
      "valid accuracy: 85.02%\n",
      "Time elapsed: 3.55 min\n",
      "Epoch: 008/015 | Batch 000/375 | Cost: 0.3749\n",
      "Epoch: 008/015 | Batch 050/375 | Cost: 0.3056\n",
      "Epoch: 008/015 | Batch 100/375 | Cost: 0.3435\n",
      "Epoch: 008/015 | Batch 150/375 | Cost: 0.3183\n",
      "Epoch: 008/015 | Batch 200/375 | Cost: 0.2402\n",
      "Epoch: 008/015 | Batch 250/375 | Cost: 0.4248\n",
      "Epoch: 008/015 | Batch 300/375 | Cost: 0.3232\n",
      "Epoch: 008/015 | Batch 350/375 | Cost: 0.2542\n",
      "training accuracy: 88.34%\n",
      "valid accuracy: 86.12%\n",
      "Time elapsed: 4.05 min\n",
      "Epoch: 009/015 | Batch 000/375 | Cost: 0.3833\n",
      "Epoch: 009/015 | Batch 050/375 | Cost: 0.3398\n",
      "Epoch: 009/015 | Batch 100/375 | Cost: 0.4420\n",
      "Epoch: 009/015 | Batch 150/375 | Cost: 0.2793\n",
      "Epoch: 009/015 | Batch 200/375 | Cost: 0.2625\n",
      "Epoch: 009/015 | Batch 250/375 | Cost: 0.2675\n",
      "Epoch: 009/015 | Batch 300/375 | Cost: 0.2263\n",
      "Epoch: 009/015 | Batch 350/375 | Cost: 0.2770\n",
      "training accuracy: 87.54%\n",
      "valid accuracy: 85.70%\n",
      "Time elapsed: 4.55 min\n",
      "Epoch: 010/015 | Batch 000/375 | Cost: 0.2831\n",
      "Epoch: 010/015 | Batch 050/375 | Cost: 0.2289\n",
      "Epoch: 010/015 | Batch 100/375 | Cost: 0.3415\n",
      "Epoch: 010/015 | Batch 150/375 | Cost: 0.2974\n",
      "Epoch: 010/015 | Batch 200/375 | Cost: 0.3623\n",
      "Epoch: 010/015 | Batch 250/375 | Cost: 0.3226\n",
      "Epoch: 010/015 | Batch 300/375 | Cost: 0.4450\n",
      "Epoch: 010/015 | Batch 350/375 | Cost: 0.3434\n",
      "training accuracy: 87.31%\n",
      "valid accuracy: 84.20%\n",
      "Time elapsed: 5.05 min\n",
      "Epoch: 011/015 | Batch 000/375 | Cost: 0.3783\n",
      "Epoch: 011/015 | Batch 050/375 | Cost: 0.2573\n",
      "Epoch: 011/015 | Batch 100/375 | Cost: 0.1965\n",
      "Epoch: 011/015 | Batch 150/375 | Cost: 0.2278\n",
      "Epoch: 011/015 | Batch 200/375 | Cost: 0.2374\n",
      "Epoch: 011/015 | Batch 250/375 | Cost: 0.2426\n",
      "Epoch: 011/015 | Batch 300/375 | Cost: 0.2344\n",
      "Epoch: 011/015 | Batch 350/375 | Cost: 0.3725\n",
      "training accuracy: 88.90%\n",
      "valid accuracy: 85.71%\n",
      "Time elapsed: 5.56 min\n",
      "Epoch: 012/015 | Batch 000/375 | Cost: 0.2915\n",
      "Epoch: 012/015 | Batch 050/375 | Cost: 0.2622\n",
      "Epoch: 012/015 | Batch 100/375 | Cost: 0.1534\n",
      "Epoch: 012/015 | Batch 150/375 | Cost: 0.2824\n",
      "Epoch: 012/015 | Batch 200/375 | Cost: 0.2519\n",
      "Epoch: 012/015 | Batch 250/375 | Cost: 0.2006\n",
      "Epoch: 012/015 | Batch 300/375 | Cost: 0.2867\n",
      "Epoch: 012/015 | Batch 350/375 | Cost: 0.2800\n",
      "training accuracy: 90.79%\n",
      "valid accuracy: 86.95%\n",
      "Time elapsed: 6.06 min\n",
      "Epoch: 013/015 | Batch 000/375 | Cost: 0.2245\n",
      "Epoch: 013/015 | Batch 050/375 | Cost: 0.1810\n",
      "Epoch: 013/015 | Batch 100/375 | Cost: 0.2121\n",
      "Epoch: 013/015 | Batch 150/375 | Cost: 0.1706\n",
      "Epoch: 013/015 | Batch 200/375 | Cost: 0.3130\n",
      "Epoch: 013/015 | Batch 250/375 | Cost: 0.2799\n",
      "Epoch: 013/015 | Batch 300/375 | Cost: 0.2522\n",
      "Epoch: 013/015 | Batch 350/375 | Cost: 0.2499\n",
      "training accuracy: 90.52%\n",
      "valid accuracy: 87.23%\n",
      "Time elapsed: 6.56 min\n",
      "Epoch: 014/015 | Batch 000/375 | Cost: 0.2326\n",
      "Epoch: 014/015 | Batch 050/375 | Cost: 0.1878\n",
      "Epoch: 014/015 | Batch 100/375 | Cost: 0.1518\n",
      "Epoch: 014/015 | Batch 150/375 | Cost: 0.2455\n",
      "Epoch: 014/015 | Batch 200/375 | Cost: 0.2472\n",
      "Epoch: 014/015 | Batch 250/375 | Cost: 0.2097\n",
      "Epoch: 014/015 | Batch 300/375 | Cost: 0.1785\n",
      "Epoch: 014/015 | Batch 350/375 | Cost: 0.3839\n",
      "training accuracy: 92.08%\n",
      "valid accuracy: 87.52%\n",
      "Time elapsed: 7.06 min\n",
      "Epoch: 015/015 | Batch 000/375 | Cost: 0.1367\n",
      "Epoch: 015/015 | Batch 050/375 | Cost: 0.3009\n",
      "Epoch: 015/015 | Batch 100/375 | Cost: 0.1925\n",
      "Epoch: 015/015 | Batch 150/375 | Cost: 0.1608\n",
      "Epoch: 015/015 | Batch 200/375 | Cost: 0.1736\n",
      "Epoch: 015/015 | Batch 250/375 | Cost: 0.3694\n",
      "Epoch: 015/015 | Batch 300/375 | Cost: 0.2564\n",
      "Epoch: 015/015 | Batch 350/375 | Cost: 0.3817\n",
      "training accuracy: 92.58%\n",
      "valid accuracy: 88.17%\n",
      "Time elapsed: 7.56 min\n",
      "Total Training Time: 7.56 min\n",
      "Test accuracy: 88.88%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        text, text_lengths = batch_data.review\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths.cpu())\n",
    "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.sentiment)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jt55pscgFdKZ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    # based on:\n",
    "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
    "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
    "    \n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    \n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "O4__q0coFJyw",
    "outputId": "1a7f84ba-a977-455e-e248-3b7036d496d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7943425476551056"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "1-predict_sentiment(model, \"This is such an awesome movie, I really love it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability negative:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8739234805107117"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability negative:')\n",
    "predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability neutral:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8013577461242676"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Probability neutral:')\n",
    "predict_sentiment(model, \"This movie was ok. It had good moments and bad moments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Default Settings:  EMBEDDING_DIM = 128,  HIDDEN_DIM = 256, LSTM  \n",
    "Total Training Time: 6.15 min  \n",
    "Test accuracy: 84.84%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Changing HIDDEN_DIM***  \n",
    "With Default Settings:  EMBEDDING_DIM = 128, LSTM  \n",
    "- Changed Settings: HIDDEN_DIM = 64,  \n",
    "Total Training Time: 3.93 min  \n",
    "Test accuracy: 86.08%  \n",
    "- Changed Settings: HIDDEN_DIM = 384,  \n",
    "Total Training Time: 8.84 min  \n",
    "Test accuracy: 86.88%  \n",
    "- Changed Settings: HIDDEN_DIM = 200,  \n",
    "Total Training Time: 5.67 min  \n",
    "Test accuracy: 87.36%  \n",
    "- Changed Settings: HIDDEN_DIM = 225,  \n",
    "Total Training Time: 5.67 min  \n",
    "Test accuracy: 87.36%  \n",
    "  \n",
    "Seems that the best accuracy comes from HIDDEN_DIM ~ 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Changing EMBEDDED_DIM***  \n",
    "With Settings:  LSTM, HIDDEN_DIM = 200\n",
    "- Changed Settings: EMBEDDING_DIM = 128,  \n",
    "Total Training Time: 5.67 min  \n",
    "Test accuracy: 87.36%  \n",
    "- Changed Settings: EMBEDDING_DIM = 100,  \n",
    "Total Training Time: 5.89 min  \n",
    "Test accuracy: 87.12%  \n",
    "- Changed Settings: EMBEDDING_DIM = 50,  \n",
    "Total Training Time: 5.58 min  \n",
    "Test accuracy: 84.56%  \n",
    "- Changed Settings: EMBEDDING_DIM = 200,  \n",
    "Total Training Time: 5.74 min  \n",
    "Test accuracy: 86.44%  \n",
    "- Changed Settings: EMBEDDING_DIM = 150,  \n",
    "Total Training Time: 6.68 min  \n",
    "Test accuracy: 85.64%  \n",
    "  \n",
    "Seems that the best accuracy comes from EMBEDDED_DIM ~ 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Changing GRU***  \n",
    "- With Settings: HIDDEN_DIM = 200, EMBEDDING_DIM = 128,  \n",
    "Total Training Time: 4.70 min  \n",
    "Test accuracy: 87.16%  \n",
    "- With Settings: HIDDEN_DIM = 256, EMBEDDING_DIM = 128,  THIS IS THE DEFAULT SETTING  \n",
    "Total Training Time: 5.55 min  \n",
    "Test accuracy: 87.68%  \n",
    "- With Settings: HIDDEN_DIM = 275, EMBEDDING_DIM = 128, \n",
    "Total Training Time: 7.20 min  \n",
    "Test accuracy: 89.44%  \n",
    "- With Settings: HIDDEN_DIM = 300, EMBEDDING_DIM = 128,   \n",
    "Total Training Time: 6.74 min  \n",
    "Test accuracy: 88.36%  \n",
    "- With Settings: HIDDEN_DIM = 275, EMBEDDING_DIM = 100,   \n",
    "Total Training Time: 8.12 min  \n",
    "Test accuracy: 88.96%  \n",
    "\n",
    "Seems that the best accuracy we can get is with GRU and HIDDEN_DIM = 275, EMBEDDING_DIM = 128."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_packed_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
